
\documentclass[12pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\linespread{1.5}
\usepackage{graphicx}
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

% See the ``Article customise'' template for come common customisations

\title{Comparing Sorting Algorithms - CS350}
\author{Russell Miller and Ben Carr}
\date{\today} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
%\tableofcontents

\section*{Introduction}
%\subsection{}

This project is an assessment of several sorting algorithms. We chose to implement four sorting algorithms: 
\begin{itemize}
\item Bubble, average  $O(n^2)$, worst-case $O(n^2)$
\item Insertion, average $O(n^2)$, worst-case $O(n^2)$
\item Quick, average $O(n$ lg $n)$ worst-case $O(n^2)$
\item Merge, average and worst-case $O(n$ lg $n)$
\end{itemize}
These four algorithms were implemented and tested in two languages: C++ and Java. We had initially planned on including Python and Haskell, 
but due to time constraints and the limitations of Python, we were only able to collect significant data and run analysis on our Java and 
C++ algorithms. Each program tested 900 completely random unsorted lists of integers, each of size $n = $ \{10, 50, 100, 500, 1000, 5000, 
10000, 50000, 100000\}. This paper will discuss our findings, assess our results and communicate to the reader our understanding of how 
complexity theory comes up in practice. We found that Quicksort ran exceptionally fast on these random lists, so additional data 
(lists size "n" up to 10,000,000 for Java and 100,000,000 for C++) was run on Quicksort.\\

\section*{Background}

We split up the main algorithms we would need to write so that each of us would be
writing them in two languages. Russell worked in Java and Python while Ben worked in Haskell and C++. Russell has
written a lot of small Python programs, and a few medium-scale ones. Java was a
major refresher for him, as it has been a while. Ben was fairly new to Haskell and hadn't used C++ much since his 
freshman year of college, so Russell was helpful when he got stuck. Writing the algorithms themselves took very 
little time, especially in Python and Haskell. These languages are so straight-forward that if you know what you 
want to do you can have something working in very little time. Later 
in this paper, we'll discuss one of Python's weak points. We also used Python to 
generate our lists and write them to files.

% struggles with java
One of the struggles we faced while working with Java was determining how 
to store the lists/arrays after reading them in from a file. At first we just wanted to have an array as a private class
field. We then discovered that most of the algorithms we were working with sorted 
in place, meaning they would destructively change the values in the array. This would cause inconsistency
for a class that has four different sort algorithms going in sequence and
accessing the same array, and unfortunately we did not abstract the sorting out 
of the primary class. Rather we used four arrays, one dedicated to each 
algorithm.

% timing issues
The next tricky thing was figuring out how to run the stopwatch on the 
algorithms while they sorted the lists. The way the Java program was laid out, we were able to place the stopwatch calls right 
next to the sort method calls, and we used public methods to retrieve the times from the private fields they were stored in. 
We also had trouble with timing our C++ algorithms. Initially we tried using the C++ function \texttt{clock()} from the library 
\texttt{sys/time.h} which gives a value for the current clock cycle. We wanted the elapsed time in seconds, in order to compare the
elapsed time to the other languages, so next tried to use \texttt{gettimeofday()}. This is a function that returns seconds and the nanoseconds in a struct.
We struggled a lot with getting readable results from the values in the struct. After researching more, we found a message board online with the following
extremely helpful hint. Using \texttt{clock()} and a C macro, we were finally getting seconds as a decimal value.
\linespread{1}
\begin{verbatim}
// source: http://www.physicsforums.com/showthread.php?t=224989
double diffclock(clock_t t1, clock_t t2)
{
  double diffticks = t2 - t1;
  double diff = diffticks / (double)CLOCKS_PER_SEC;
  return diff;
}
\end{verbatim}
\linespread{1.5}
The final struggle while working in Java, which carried over to the C++ implementation as well, was the merge sort. The way we understood the merge sort algorithm
was that it split the list into two smaller lists and recursed down to those. Then
the merge action put the smaller lists back together. Algorithms exist to do this in place with an array, but the easier implementation was to try the 
Java \texttt{List} class. In attempting to learn to use it, we also discovered the magic of the \texttt{ArrayList} class. Basically, it
allows you to do list operations such as \texttt{cons} (Java calls it \texttt{add}), and there is also a \texttt{get} function that allows 
you to refer to a specific index. Both of these operations came in handy. C++ has lists in its standard library, and though it did 
not have the get method, it was not difficult to make the translation from the Java code to the C++ code.

% struggles with list sizes
Once we verified that all of the sorts were in fact sorting, adjusted the output
that would be written to the CSV file, tested the stopwatch, and did a dry-run
on a small set of lists, it was time to ship it. The program ran for two days
and still had not finished. At this point we became very worried, so we tried
timing a single list of size 100,000. In java it only took about 30 seconds, but
in C++ it took 3 minutes. We later discovered that this enormous difference in time
for these very similar languages was due to leaving some extra asserts in the code
and forgetting to comment them out for the stopwatch trials. Still, at this point
we decided to reduce our list size limit to 100,000 instead of one million.

\section*{Understanding Algorithms}
% understanding algorithms
While putting these sorting algorithms into our programs, one thing that helped
us understand them was to go through them, line by line, with an example on a
dry erase board. In addition, we made sure to test them all and verify that they
were doing their job. The first sort we studied was the Quicksort. It turns out
it is a relatively simple algorithm. Figure 1 has a Quicksort example. When looking at an array of numbers, you simply pick a value that is present
\begin{center}
    \includegraphics[width=.6\textwidth]{Quicksort-drawing.jpg}\\
    {\scshape Figure 1.} Example of Quicksort
\end{center}
 in the array - the pivot - and divide the 
rest of the array based on whether the values are less than or greater than the 
pivot. It's recursive so you then apply this to those two new arrays.


The Quicksort is a strange beast. When compared with the complexity of other
sorting algorithms it doesn't seem to be so great because of the worst case $O(n^2)$ time that it can take. However, it is extremely unlikely for that case to take place. Its performance is also very good on lists that are not "nearly-sorted" which was the type of data we were feeding it.  The behavior of Quicksort for smaller n seems to be much faster than that of the algorithms which do not have as high of a worst-case condition. The trickiest part is getting a good pivot value. 
\begin{center}
    \includegraphics[width=.7\textwidth]{Quicksort-worstcase.jpg}\\
    {\scshape Figure 2.} Example of Quicksort's worst-case
\end{center}
By getting a lucky pivot value the
divide portion of this divide-and-conquer algorithm is more effective. The
reason $O(n^2)$ is possible is that the recursion only stops when the list being
sorted has been reduced to size one. So if every pivot value is the worst
possible pivot value, every divide will result in an empty list, and an n-1
sized list. Figure 2 is a hand-drawn example of Quicksort's worst-case.

% understanding algorithms - complexity of Quicksort
The complexity of Quicksort can vary considerably depending on the layout of the
data being sorted and on how the pivot value is picked. Carefully picking a
pivot value can be considered optimization and in our algorithms we didn't do
anything special to pick ours. This is a divide and conquer algorithm, and on
average can do O(n logn). The conquer part is a platform-specific complexity
because sometimes concatenating lists or arrays together can be constant, but is
usually a linear operation. The divide happens around a pivot value, and if the
pivot value is greater than half of the array and less than the other half there
will be a logarithmic complexity for the divide. If the pivot value causes one
side of the division to be a lot larger than the other, the complexity moves
toward quadratic. The nice thing about this algorithm, though, is it doesn't
need to do as many comparisons as Merge Sort. Once it has done its full
recursion it is simply concatenating results together. This may be the primary
reason for Quicksort's victory.


% understanding algorithms - insertion sort
The insertion sort is a particularly bad algorithm. What it does is iterate
through the list, starting at the second element. This iteration is $O(n)$. For
each iteration, There is also a while loop that starts at that point and goes
backwards towards the beginning of the list swapping values that are out of
order. Figure 3 shows how the "key" value is iterating across the
array, and each time a new "key" is assigned a value, the while loop causes
swaps towards the beginning. In asymptotic terms, the while loop is also $O(n)$.
Most of the time, the while loop that happens after each new
 value is assigned
to "key" does not actually traverse all the way to the beginning of the array,
so the worst-case condition is not highly probable. 
% Insertion Sort Walkthrough
  \begin{center}
      \includegraphics[width=.7\textwidth]{insertion-drawing.jpg}\\
    {\scshape Figure 3.} Example of Insertion Sort.
	\end{center}

On a sorted list, the
condition to enter the while loop is always false and the complexity is $O(n)$ for
the entire sort, because it is handled with just the for loop. In all other lists,
however, the complexity is $O(n)$ for the for loop, times $O(n)$ for the while loop,
resulting in $O(n^2)$.

The easiest algorithm to implement was bubble sort. The complexity is average case$O(n^2)$, as is shown in our comparison graphs. The reason that the complexity is so bad is as follows. It walks along the array/list (which is size n) and compares each item in order. When it gets through the list, it starts over, and on average has to walk through the list n times because it's only fixing (more or less) one thing at a time each time it walks through. I feel that detailed comments are the best way I can explain how bubble sort works:
\linespread{1}

\begin{verbatim}
void arrObj::bubbleSort()
{
    int swap;
    do{
        swap = 0; // swap is like a bool, it tells do-while 
                  //to loop if the list is not sorted
                  
        for (int n = 1; n < arrSize; n++) {
        // each time we enter into the do-while loop, the
        // for loop traverses the entire array, comparing
        // array[n-1] to array[n]. When it finds that 
        // array[n-1] is greater than array[n], 
        // it first sets swap to 1 (so we do-while again, 
        // then it swaps. In my implementation, I use the 
        // variable temp to store array[n-1]s valueÉ 
        // copy the value in array[n] to array[n-1]
        // and then finally copy temps value to array[n].
		
            if (arrBubble[n-1] > arrBubble[n]) {
                swap = 1;
                int temp = arrBubble[n-1];
                arrBubble[n-1] = arrBubble[n];
                arrBubble[n] = temp;
            }
        }
    }while(swap == 1);
         // When we finally get here, if swap is set to 1, 
         // we have to walk the entire array again, doing
         // the for loop. Eventually
         // we will sort the list.
    //assert(isSorted(arrBubble));
}
\end{verbatim}

% Bubble Sort Walkthrough
  \begin{center}
    \includegraphics[width=.7\textwidth]{bSortWalk.png}\\
 {\scshape Figure 4.} Example of Bubble sort
\end{center}

\newpage



% merge Sort Walkthrough
\begin{center}
    \includegraphics[width=.7\textwidth]{mergeSortdrawing.jpg}\\
  {\scshape Figure 5.} Example of Merge sort
\end{center}
Figure 4. has a walk-through of bubble Sort on a small array, to show how it works.

Merge sort is much faster than bubble sort. We have included a walk through of merge sort in Figure 5. Merge sort takes a list and keeps splitting it into smaller lists until each list has one element in it (as seen in the top half of FIgure 5. This dividing takes $O($log $n$) time, because you're dividing in half every time. Ss you recurse back up the tree, each merge of a size r list and a size s list takes $r+s-1$ comparisons, which is $O(r+s)$ The final merge will be $r+s$, which is the size of the list, $n$. When you combine these you get $O(n$ log $n$).

%Findings/ANAlysis
\section*{Findings and Analysis}
Our C++ and Java programs took a total elapsed time of about 2 hours to sort all of the lists, 
sizes $n = $ \{10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000\}. For each of these sizes, 
100 different lists were sorted by each algorithm (the total number of lists the program sorted was $100 * |n| = 900)$. 
The average time for each algorithm (in seconds) was recorded for Java and C++ and can be found in Table 1 and Table 2 respectively. Figure 6 and Figure 7 are the graphs of these averages. We wanted to see how consistent the algorithms were behaving; Figure 8 and Figure 9 show each trial's 
execution time for each algorithm. Quicksort ran incredibly fast, so we decided to run it on larger 
lists (up to n = 100,000,000). Figure 10 shows the Quicksort behavior against the calculated $n$ log $n$ plot.
Note that in C++ our values of $n$ were much higher. When attempting those values with Java, the virtual machine
ran out of memory and threw an exception so a lower limit was used.
\newpage
%Java Table
\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ r | l l l l  }

List Size	&	Quick	&	Bubble	&	Insertion	&	Merge\\ \hline
10			&	0		&	0		&	0			&	0\\
50			&	0		&	0		&	0			&	0\\
100			&	0		&	0		&	0			&	0\\
500			&	0		&	0		&	0			&	0\\
1000			&	0		&	0		&	0			&	0\\
5000			&	0		&	0.05		&	0			&	0.01\\
10000		&	0		&	0.19		&	0.01			&	0.03\\
50000		&	0		&	4.84		&	0.37			&	0.72\\
100000		&	0.01		&	19.4		&	1.51			&	3.03\\

\end{tabular}
\end{center}
\caption{Java average run times (in seconds) of unordered lists.}
\end{table}

%Java All Sorts
\begin{center}
    \includegraphics[width=0.9\textwidth]{JAllSorts.png}\\
  {\scshape Figure 6} Behavior of sorts against each other in Java
\end{center}
%C++ Table
\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ r | l l l l  }

List Size	&	Quick	&	Bubble	&	Insertion	&	Merge \\ \hline
10			&	0		&	0		&	0			&	0\\
50			&	0		&	0		&	0			&	0\\
100			&	0		&	0		&	0			&	0\\
500			&	0		&	0		&	0			&	0\\
1000 		&	0		&	0		&	0			&	0\\
5000			&	0		&	0.06		&	0.01			&	0\\
10000		&	0		&	0.24		&	0.03			&	0\\
50000		&	0		&	5.93		&	0.81			&	9.54E-006\\
100000		&	0		&	23.75	&	3.29			&	5.40E-005\\

\end{tabular}
\end{center}
\caption{C++ average run times (in seconds) of unordered lists.}
\end{table}
%C++ All Sorts
\begin{center}
    \includegraphics[width=0.9\textwidth]{CAllSorts.png}\\
  {\scshape Figure 7.} Behavior of sorts against each other in C++
\end{center}

\newpage %Consistency Graphs
\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{JQuickCG.png}\\
\end{center}

\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{JBubbleCG.png}\\
\end{center}
\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{JInsertionCG.png}\\
\end{center}
\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{JMergeSortCG.png}\\
    {\scshape Figure 8.} Java Sort consistency graphs
\end{center}

\newpage %Consistency Graphs
\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{CQuickCG.png}\\
\end{center}

\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{CBubbleCG.png}\\
\end{center}
\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{CInsertionCG.png}\\
\end{center}
\begin{center}
  \centering
    \includegraphics[width=1.0\textwidth]{CMergeCG.png}\\
    {\scshape Figure 9.} C++ Sort consistency graphs
\end{center}
In Figure 9, it is clear that Quicksort was consistent on all but 2 of the arrays it sorted. This is a clear example of how Quicksort can suffer in rare cases.
Because we do not have the ability to look at a long list of numbers and recognize patterns it is not quite as clear \emph{why} the times for those 2 arrays were
greater, but we do have physicaly verification of the lack of consistency that is present with Quicksort. All of the other sorts were relatively smooth across
these graphs.
\newpage



\newpage
\newpage %Hardcore Quicksort
\begin{center}
  \centering
    \includegraphics[width=.7\textwidth]{JHardcoreQS.png}\\
\end{center}

\begin{center}
  \centering
    \includegraphics[width=.7\textwidth]{CHardcoreQS.png}\\
  {\scshape Figure 10.} Behavior of Quicksort on large $n$
\end{center}
\newpage

\section*{Testing Methods and List Generation Strategies}
% testing
In order to verify that our algorithms were sound, we implemented a good deal of testing. The test set we used while developing was 100 lists of size 10. In order to
verify that a list was sorted after an algorithm ran, we used assertions to make sure that regression was never introduced. In Java, there was a method available
to sort an Array. After running all of the sorts, we created a test copy of the original array data, ran the Java array sort on it, and iterated through each
array our algorithms sorted making sure that each value of the arrays matched. Not only does this verify that the array is sorted, but it also verifies that no values
have mysteriously disappeared from the original data. C++ was not quite as elegant. We simply iterated through the arrays that we believed were sorted, and made sure
each value was less than its successor. Not only were we preventive with making sure our algorithms were sound, but we ran our timers multiple times due to the results
that were gathered. We knew something was wrong when C++ was taking four times longer than Java, and further investigation showed us that there were assertions being run
that should have been disabled while the timer was going. Our final run had results that were much more realistic, where C++ and Java took a very similar amount of time.
% generating the lists
We decided that it would be good to have a consistent set of lists that are 
randomly generated to use across all of the sorting algorithms and all of the 
languages. The reason behind this was our plan of comparing the implementations across languages. That is, if we wanted to compare one language to another, it would be unfair to compare their performance if they were operating on different data. There were many struggles when we were developing this list generator.
It took several tries to figure out how to build up the right amount of data in 
memory before writing it to a file, because we did not want to spend extra time 
doing more File I/O than necessary. 

At first we started with a very simple program that when given an argument, would
create 1000 new files, each containing a list of the size specified in its 
argument. This worked fine for the first few sizes, so we ran the script in 
separate shells, with different inputs, to get all of our lists ready. However,
the system ran out of memory before it could finish.

Next we decided to write each list to a file, one by one, rather than storing all 
of the information in memory and attempting to write all of the files at the end. 
This worked, but it took quite a considerable amount of time. 
We also had difficulty with the disk space all of these lists took up.
We eventually reduced the number of each size of list from 1000 to 100.


\section*{Conclusions/Reflections}
% reflections on our data
As the data has clearly shown, and was discussed earlier, Quicksort was the
reigning champion of our small collection of algorithms. While we were in class
we learned about Merge Sort and how it does not have a worst-case complexity as
high as Quicksort does. This led us to believe that Merge Sort would be the
fastest of the four, however our results showed that not to be the case in practice.

As discussed earlier, we had to take our number-of-list size down to 100 which is an example of how complexity theory shows up in practice. We also set out to sort lists of size up to 1,000,000. After running our algorithms for several hours, it became apparent that the programs were not going to terminate in time, most likely due to the worst sorting algorithm we implemented, bubble sort with an average $O(n^2)$ behavior. So, in practice, to prepare usable data, we had to consider the complexity an alter our process so we would have usable data in enough time to run analysis for this project.
\newpage
We think the reason that Quicksort worked so well for our data was because the data we were giving it was totally random. Had we chosen to feed nearly sorted or sorted lists to our algorithms, we would have seen drastically different results. For example Quicksort would have taken much longer, and bubble sort would have performed much more effectively.

\end{document}

